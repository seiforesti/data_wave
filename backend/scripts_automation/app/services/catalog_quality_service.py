"""
ðŸŽ¯ CATALOG QUALITY SERVICE
==========================

Enterprise-grade data catalog quality management service that provides:
- Comprehensive data quality assessment and scoring
- Real-time quality monitoring and alerting
- Automated quality rule engines and validation
- Quality reporting and dashboards
- Quality improvement recommendations and remediation

This service integrates with catalog discovery and lineage to provide
complete data quality governance across the enterprise catalog.
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Set, Union
from collections import defaultdict
from dataclasses import dataclass, field
from enum import Enum
from statistics import mean, median, stdev

import pandas as pd
import numpy as np
from sqlmodel import SQLModel, Field, select, and_, or_
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload
from pydantic import BaseModel, validator

from ..core.database import get_async_session
from ..models.scan_models import (
    DataSource, ScanResult, Scan, QualityMetric, GrowthMetric,
    DataSourceType, ScanStatus
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QualityDimension(str, Enum):
    """Data quality dimensions"""
    COMPLETENESS = "completeness"           # Data completeness/nulls
    ACCURACY = "accuracy"                   # Data accuracy/correctness
    CONSISTENCY = "consistency"             # Data consistency/uniformity
    VALIDITY = "validity"                   # Data validity/format compliance
    UNIQUENESS = "uniqueness"               # Data uniqueness/duplicates
    TIMELINESS = "timeliness"              # Data freshness/currency
    INTEGRITY = "integrity"                 # Referential integrity
    CONFORMITY = "conformity"              # Schema/format conformity


class QualityIssueType(str, Enum):
    """Types of quality issues"""
    MISSING_VALUE = "missing_value"         # Null/empty values
    INVALID_FORMAT = "invalid_format"       # Format violations
    DUPLICATE_RECORD = "duplicate_record"   # Duplicate data
    OUTLIER = "outlier"                    # Statistical outliers
    CONSTRAINT_VIOLATION = "constraint_violation"  # Business rule violations
    SCHEMA_MISMATCH = "schema_mismatch"    # Schema inconsistencies
    STALE_DATA = "stale_data"              # Outdated data
    ORPHAN_RECORD = "orphan_record"        # Referential integrity issues


class QualityRuleSeverity(str, Enum):
    """Severity levels for quality rules"""
    CRITICAL = "critical"                   # System breaking issues
    HIGH = "high"                          # Significant data issues
    MEDIUM = "medium"                      # Moderate data issues
    LOW = "low"                           # Minor data issues
    INFO = "info"                         # Informational only


class QualityAssessmentScope(str, Enum):
    """Scope of quality assessment"""
    CATALOG_WIDE = "catalog_wide"          # Entire catalog assessment
    DATA_SOURCE = "data_source"            # Single data source
    SCHEMA = "schema"                      # Database schema level
    TABLE = "table"                        # Individual table
    COLUMN = "column"                      # Column level assessment


@dataclass
class QualityRule:
    """Represents a data quality rule"""
    rule_id: str
    rule_name: str
    description: str
    dimension: QualityDimension
    severity: QualityRuleSeverity
    rule_logic: str
    threshold: Optional[float] = None
    is_active: bool = True
    created_at: Optional[datetime] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class QualityIssue:
    """Represents a data quality issue"""
    issue_id: str
    issue_type: QualityIssueType
    dimension: QualityDimension
    severity: QualityRuleSeverity
    description: str
    affected_entity: str  # table.column or table
    data_source_id: int
    rule_id: Optional[str] = None
    detected_at: Optional[datetime] = None
    sample_values: List[Any] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class QualityScore:
    """Represents quality scores for different dimensions"""
    overall_score: float
    dimension_scores: Dict[QualityDimension, float] = field(default_factory=dict)
    issue_count: int = 0
    critical_issues: int = 0
    timestamp: Optional[datetime] = None
    confidence_level: float = 1.0


class QualityAssessmentResult(BaseModel):
    """Results of a quality assessment"""
    assessment_id: str
    scope: QualityAssessmentScope
    target_entity: str
    quality_score: QualityScore
    issues_found: List[QualityIssue]
    recommendations: List[str]
    assessment_timestamp: datetime
    processing_stats: Dict[str, Any]


class QualityTrendAnalysis(BaseModel):
    """Quality trend analysis over time"""
    entity_id: str
    time_period: str
    trend_direction: str  # improving, declining, stable
    quality_evolution: List[Dict[str, Any]]
    key_insights: List[str]
    forecasted_quality: Optional[float]
    analysis_timestamp: datetime


class CatalogQualityService:
    """
    ðŸŽ¯ Catalog Quality Management Service
    
    Provides comprehensive data quality assessment, monitoring, and 
    improvement capabilities for the enterprise data catalog.
    """
    
    def __init__(self):
        self.logger = logger
        self.quality_rules: Dict[str, QualityRule] = {}
        self.quality_profiles: Dict[str, Dict[str, Any]] = {}
        self.quality_history: Dict[str, List[QualityScore]] = {}
        self._initialize_service()
    
    def _initialize_service(self):
        """Initialize the quality service with default rules and configurations"""
        self.logger.info("ðŸŽ¯ Initializing Catalog Quality Service")
        
        # Initialize default quality rules
        self._initialize_default_quality_rules()
        
        # Initialize quality profiles
        self.quality_profiles = {
            "standard": {
                "completeness_threshold": 0.95,
                "accuracy_threshold": 0.90,
                "consistency_threshold": 0.85,
                "validity_threshold": 0.90,
                "uniqueness_threshold": 0.99
            },
            "strict": {
                "completeness_threshold": 0.99,
                "accuracy_threshold": 0.95,
                "consistency_threshold": 0.90,
                "validity_threshold": 0.95,
                "uniqueness_threshold": 0.999
            },
            "relaxed": {
                "completeness_threshold": 0.85,
                "accuracy_threshold": 0.80,
                "consistency_threshold": 0.75,
                "validity_threshold": 0.80,
                "uniqueness_threshold": 0.95
            }
        }
        
        self.logger.info("âœ… Catalog Quality Service initialized successfully")

    def _initialize_default_quality_rules(self):
        """Initialize default quality rules"""
        default_rules = [
            QualityRule(
                rule_id="completeness_check",
                rule_name="Data Completeness Check",
                description="Check for missing/null values in critical columns",
                dimension=QualityDimension.COMPLETENESS,
                severity=QualityRuleSeverity.HIGH,
                rule_logic="null_percentage < threshold",
                threshold=0.05
            ),
            QualityRule(
                rule_id="uniqueness_check",
                rule_name="Data Uniqueness Check",
                description="Check for duplicate values in unique columns",
                dimension=QualityDimension.UNIQUENESS,
                severity=QualityRuleSeverity.HIGH,
                rule_logic="duplicate_percentage < threshold",
                threshold=0.01
            ),
            QualityRule(
                rule_id="format_validation",
                rule_name="Data Format Validation",
                description="Validate data format compliance",
                dimension=QualityDimension.VALIDITY,
                severity=QualityRuleSeverity.MEDIUM,
                rule_logic="format_compliance > threshold",
                threshold=0.95
            ),
            QualityRule(
                rule_id="freshness_check",
                rule_name="Data Freshness Check",
                description="Check data currency and timeliness",
                dimension=QualityDimension.TIMELINESS,
                severity=QualityRuleSeverity.MEDIUM,
                rule_logic="days_since_update < threshold",
                threshold=7.0
            ),
            QualityRule(
                rule_id="consistency_check",
                rule_name="Data Consistency Check",
                description="Check for data consistency across related entities",
                dimension=QualityDimension.CONSISTENCY,
                severity=QualityRuleSeverity.MEDIUM,
                rule_logic="consistency_score > threshold",
                threshold=0.90
            )
        ]
        
        for rule in default_rules:
            rule.created_at = datetime.utcnow()
            self.quality_rules[rule.rule_id] = rule

    async def assess_catalog_quality(
        self,
        scope: QualityAssessmentScope,
        target_entity: str,
        quality_profile: str = "standard",
        assessment_config: Optional[Dict[str, Any]] = None
    ) -> QualityAssessmentResult:
        """
        ðŸ” Perform comprehensive quality assessment on catalog entities
        
        Args:
            scope: Scope of assessment (catalog_wide, data_source, table, etc.)
            target_entity: Target entity identifier
            quality_profile: Quality profile to use (standard, strict, relaxed)
            assessment_config: Optional assessment configuration
            
        Returns:
            QualityAssessmentResult with complete assessment details
        """
        assessment_id = str(uuid.uuid4())
        self.logger.info(f"ðŸ” Starting quality assessment: {assessment_id}")
        
        try:
            config = assessment_config or {}
            profile = self.quality_profiles.get(quality_profile, self.quality_profiles["standard"])
            
            start_time = datetime.utcnow()
            
            # Get data for assessment based on scope
            assessment_data = await self._get_assessment_data(scope, target_entity, config)
            
            # Run quality checks
            quality_issues = await self._run_quality_checks(
                assessment_data, profile, config
            )
            
            # Calculate quality scores
            quality_score = await self._calculate_quality_scores(
                assessment_data, quality_issues, profile
            )
            
            # Generate recommendations
            recommendations = await self._generate_quality_recommendations(
                quality_issues, quality_score, profile, config
            )
            
            # Calculate processing statistics
            processing_stats = {
                "records_assessed": len(assessment_data.get("records", [])),
                "rules_executed": len(self.quality_rules),
                "issues_found": len(quality_issues),
                "processing_time_seconds": (datetime.utcnow() - start_time).total_seconds(),
                "data_sources_covered": len(set(
                    issue.data_source_id for issue in quality_issues
                ))
            }
            
            # Store quality score in history
            await self._store_quality_history(target_entity, quality_score)
            
            result = QualityAssessmentResult(
                assessment_id=assessment_id,
                scope=scope,
                target_entity=target_entity,
                quality_score=quality_score,
                issues_found=quality_issues,
                recommendations=recommendations,
                assessment_timestamp=datetime.utcnow(),
                processing_stats=processing_stats
            )
            
            self.logger.info(f"âœ… Quality assessment completed: {assessment_id}")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Error in quality assessment: {str(e)}")
            raise

    async def monitor_quality_trends(
        self,
        entity_id: str,
        time_period_days: int = 30,
        analysis_config: Optional[Dict[str, Any]] = None
    ) -> QualityTrendAnalysis:
        """
        ðŸ“ˆ Analyze quality trends over time for an entity
        
        Args:
            entity_id: Entity identifier to analyze
            time_period_days: Number of days to analyze
            analysis_config: Optional analysis configuration
            
        Returns:
            QualityTrendAnalysis with trend insights and forecasts
        """
        self.logger.info(f"ðŸ“ˆ Analyzing quality trends for entity: {entity_id}")
        
        try:
            config = analysis_config or {}
            
            # Get historical quality data
            quality_history = await self._get_quality_history(
                entity_id, time_period_days
            )
            
            if len(quality_history) < 2:
                return QualityTrendAnalysis(
                    entity_id=entity_id,
                    time_period=f"{time_period_days} days",
                    trend_direction="insufficient_data",
                    quality_evolution=[],
                    key_insights=["Insufficient historical data for trend analysis"],
                    forecasted_quality=None,
                    analysis_timestamp=datetime.utcnow()
                )
            
            # Analyze trend direction
            trend_direction = await self._analyze_trend_direction(quality_history)
            
            # Generate quality evolution timeline
            quality_evolution = await self._generate_quality_evolution(quality_history)
            
            # Extract key insights
            key_insights = await self._extract_quality_insights(
                quality_history, trend_direction, config
            )
            
            # Forecast future quality
            forecasted_quality = await self._forecast_quality(quality_history, config)
            
            trend_analysis = QualityTrendAnalysis(
                entity_id=entity_id,
                time_period=f"{time_period_days} days",
                trend_direction=trend_direction,
                quality_evolution=quality_evolution,
                key_insights=key_insights,
                forecasted_quality=forecasted_quality,
                analysis_timestamp=datetime.utcnow()
            )
            
            return trend_analysis
            
        except Exception as e:
            self.logger.error(f"âŒ Error in trend analysis: {str(e)}")
            raise

    async def create_quality_rule(
        self,
        rule_definition: Dict[str, Any]
    ) -> QualityRule:
        """
        âž• Create a new custom quality rule
        
        Args:
            rule_definition: Rule definition dictionary
            
        Returns:
            Created QualityRule object
        """
        self.logger.info("âž• Creating new quality rule")
        
        try:
            rule = QualityRule(
                rule_id=rule_definition.get("rule_id", str(uuid.uuid4())),
                rule_name=rule_definition["rule_name"],
                description=rule_definition["description"],
                dimension=QualityDimension(rule_definition["dimension"]),
                severity=QualityRuleSeverity(rule_definition["severity"]),
                rule_logic=rule_definition["rule_logic"],
                threshold=rule_definition.get("threshold"),
                is_active=rule_definition.get("is_active", True),
                created_at=datetime.utcnow(),
                metadata=rule_definition.get("metadata", {})
            )
            
            # Validate rule logic
            await self._validate_rule_logic(rule)
            
            # Store the rule
            self.quality_rules[rule.rule_id] = rule
            
            self.logger.info(f"âœ… Quality rule created: {rule.rule_name}")
            return rule
            
        except Exception as e:
            self.logger.error(f"âŒ Error creating quality rule: {str(e)}")
            raise

    async def get_quality_dashboard_data(
        self,
        dashboard_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ðŸ“Š Generate data for quality dashboard
        
        Args:
            dashboard_config: Optional dashboard configuration
            
        Returns:
            Dictionary containing dashboard data and metrics
        """
        self.logger.info("ðŸ“Š Generating quality dashboard data")
        
        try:
            config = dashboard_config or {}
            
            async with get_async_session() as session:
                # Get overall catalog statistics
                catalog_stats = await self._get_catalog_statistics(session)
                
                # Get recent quality assessments
                recent_assessments = await self._get_recent_assessments(session, limit=10)
                
                # Get top quality issues
                top_issues = await self._get_top_quality_issues(session, limit=10)
                
                # Get quality trends
                quality_trends = await self._get_quality_trend_summary(session)
                
                # Get data source quality rankings
                source_rankings = await self._get_data_source_quality_rankings(session)
                
                # Generate quality alerts
                quality_alerts = await self._generate_quality_alerts(session)
                
                dashboard_data = {
                    "overview": {
                        "total_data_sources": catalog_stats["total_data_sources"],
                        "total_tables": catalog_stats["total_tables"],
                        "total_columns": catalog_stats["total_columns"],
                        "overall_quality_score": catalog_stats["avg_quality_score"],
                        "active_quality_rules": len([r for r in self.quality_rules.values() if r.is_active])
                    },
                    "recent_assessments": recent_assessments,
                    "top_issues": top_issues,
                    "quality_trends": quality_trends,
                    "source_rankings": source_rankings,
                    "alerts": quality_alerts,
                    "quality_dimensions": await self._get_dimension_summary(session),
                    "timestamp": datetime.utcnow()
                }
                
                return dashboard_data
                
        except Exception as e:
            self.logger.error(f"âŒ Error generating dashboard data: {str(e)}")
            raise

    async def remediate_quality_issues(
        self,
        issue_ids: List[str],
        remediation_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ðŸ”§ Provide remediation suggestions and automated fixes for quality issues
        
        Args:
            issue_ids: List of quality issue IDs to remediate
            remediation_config: Optional remediation configuration
            
        Returns:
            Dictionary containing remediation results and actions taken
        """
        self.logger.info(f"ðŸ”§ Remediating {len(issue_ids)} quality issues")
        
        try:
            config = remediation_config or {}
            remediation_results = {
                "remediation_id": str(uuid.uuid4()),
                "issues_processed": len(issue_ids),
                "auto_fixed": 0,
                "manual_actions_required": 0,
                "remediation_actions": [],
                "recommendations": [],
                "timestamp": datetime.utcnow()
            }
            
            for issue_id in issue_ids:
                # Get issue details (in production, this would fetch from database)
                issue_details = await self._get_issue_details(issue_id)
                
                if not issue_details:
                    continue
                
                # Generate remediation plan
                remediation_plan = await self._generate_remediation_plan(
                    issue_details, config
                )
                
                # Attempt automated remediation if configured
                if config.get("auto_remediate", False):
                    auto_result = await self._attempt_auto_remediation(
                        issue_details, remediation_plan
                    )
                    
                    if auto_result["success"]:
                        remediation_results["auto_fixed"] += 1
                    else:
                        remediation_results["manual_actions_required"] += 1
                else:
                    remediation_results["manual_actions_required"] += 1
                
                remediation_results["remediation_actions"].append({
                    "issue_id": issue_id,
                    "issue_type": issue_details.get("issue_type"),
                    "remediation_plan": remediation_plan,
                    "auto_remediated": config.get("auto_remediate", False)
                })
            
            # Generate overall recommendations
            overall_recommendations = await self._generate_overall_remediation_recommendations(
                issue_ids, remediation_results
            )
            remediation_results["recommendations"] = overall_recommendations
            
            self.logger.info(f"âœ… Remediation completed: {remediation_results['remediation_id']}")
            return remediation_results
            
        except Exception as e:
            self.logger.error(f"âŒ Error in quality remediation: {str(e)}")
            raise

    # ===================== PRIVATE HELPER METHODS =====================

    async def _get_assessment_data(
        self,
        scope: QualityAssessmentScope,
        target_entity: str,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Get data for quality assessment based on scope"""
        async with get_async_session() as session:
            if scope == QualityAssessmentScope.DATA_SOURCE:
                # Get data source and its scan results
                query = select(DataSource).where(DataSource.id == int(target_entity))
                result = await session.execute(query)
                data_source = result.scalar_one_or_none()
                
                if not data_source:
                    return {"records": [], "metadata": {}}
                
                # Get scan results for this data source
                scan_query = (
                    select(ScanResult)
                    .join(Scan)
                    .where(Scan.data_source_id == data_source.id)
                    .limit(config.get("sample_size", 1000))
                )
                scan_results = await session.execute(scan_query)
                
                return {
                    "records": scan_results.scalars().all(),
                    "metadata": {
                        "data_source": data_source,
                        "scope": scope.value,
                        "entity_id": target_entity
                    }
                }
            
            # Handle other assessment scopes with real data retrieval
            async with get_async_session() as session:
                if scope == QualityAssessmentScope.DATA_SOURCE:
                    # Get data source and related information
                    data_source_result = await session.execute(
                        select(DataSource).where(DataSource.id == int(target_entity))
                    )
                    data_source = data_source_result.scalar_one_or_none()
                    
                    if data_source:
                        # Get recent scan results for this data source
                        scan_results_query = await session.execute(
                            select(ScanResult).join(Scan).where(
                                Scan.data_source_id == data_source.id
                            ).order_by(desc(ScanResult.created_at)).limit(1000)
                        )
                        scan_results = scan_results_query.scalars().all()
                        
                        return {
                            "records": [
                                {
                                    "id": str(result.id),
                                    "schema_name": result.schema_name,
                                    "table_name": result.table_name,
                                    "column_name": result.column_name,
                                    "data_type": result.data_type,
                                    "nullable": result.nullable,
                                    "classification_labels": result.classification_labels or [],
                                    "sensitivity_level": result.sensitivity_level,
                                    "scan_metadata": result.scan_metadata or {},
                                    "created_at": result.created_at.isoformat() if result.created_at else None
                                }
                                for result in scan_results
                            ],
                            "metadata": {
                                "scope": scope.value,
                                "entity_id": target_entity,
                                "data_source_name": data_source.name,
                                "data_source_type": data_source.source_type.value,
                                "total_records": len(scan_results)
                            }
                        }
                
                elif scope == QualityAssessmentScope.SCHEMA:
                    # Get schema-level data from scan results
                    schema_parts = target_entity.split('.')
                    if len(schema_parts) >= 2:
                        data_source_id, schema_name = schema_parts[0], schema_parts[1]
                        
                        scan_results_query = await session.execute(
                            select(ScanResult).join(Scan).where(
                                and_(
                                    Scan.data_source_id == int(data_source_id),
                                    ScanResult.schema_name == schema_name
                                )
                            ).order_by(desc(ScanResult.created_at)).limit(500)
                        )
                        scan_results = scan_results_query.scalars().all()
                        
                        return {
                            "records": [
                                {
                                    "id": str(result.id),
                                    "table_name": result.table_name,
                                    "column_name": result.column_name,
                                    "data_type": result.data_type,
                                    "nullable": result.nullable,
                                    "scan_metadata": result.scan_metadata or {}
                                }
                                for result in scan_results
                            ],
                            "metadata": {
                                "scope": scope.value,
                                "entity_id": target_entity,
                                "schema_name": schema_name,
                                "data_source_id": data_source_id,
                                "total_records": len(scan_results)
                            }
                        }
                
                elif scope == QualityAssessmentScope.TABLE:
                    # Get table-level data
                    table_parts = target_entity.split('.')
                    if len(table_parts) >= 3:
                        data_source_id, schema_name, table_name = table_parts[0], table_parts[1], table_parts[2]
                        
                        scan_results_query = await session.execute(
                            select(ScanResult).join(Scan).where(
                                and_(
                                    Scan.data_source_id == int(data_source_id),
                                    ScanResult.schema_name == schema_name,
                                    ScanResult.table_name == table_name
                                )
                            ).order_by(desc(ScanResult.created_at)).limit(100)
                        )
                        scan_results = scan_results_query.scalars().all()
                        
                        return {
                            "records": [
                                {
                                    "id": str(result.id),
                                    "column_name": result.column_name,
                                    "data_type": result.data_type,
                                    "nullable": result.nullable,
                                    "scan_metadata": result.scan_metadata or {}
                                }
                                for result in scan_results
                            ],
                            "metadata": {
                                "scope": scope.value,
                                "entity_id": target_entity,
                                "table_name": table_name,
                                "schema_name": schema_name,
                                "data_source_id": data_source_id,
                                "total_records": len(scan_results)
                            }
                        }
                
                # Fallback for unrecognized scopes
                return {
                    "records": [],
                    "metadata": {
                        "scope": scope.value,
                        "entity_id": target_entity,
                        "error": "Scope not fully implemented"
                    }
                }

    async def _run_quality_checks(
        self,
        assessment_data: Dict[str, Any],
        profile: Dict[str, float],
        config: Dict[str, Any]
    ) -> List[QualityIssue]:
        """Run quality checks on assessment data"""
        issues = []
        records = assessment_data.get("records", [])
        metadata = assessment_data.get("metadata", {})
        
        if not records:
            return issues
        
        # Run completeness checks
        completeness_issues = await self._check_completeness(records, metadata, profile)
        issues.extend(completeness_issues)
        
        # Run uniqueness checks
        uniqueness_issues = await self._check_uniqueness(records, metadata, profile)
        issues.extend(uniqueness_issues)
        
        # Run validity checks
        validity_issues = await self._check_validity(records, metadata, profile)
        issues.extend(validity_issues)
        
        # Run consistency checks
        consistency_issues = await self._check_consistency(records, metadata, profile)
        issues.extend(consistency_issues)
        
        # Run timeliness checks
        timeliness_issues = await self._check_timeliness(records, metadata, profile)
        issues.extend(timeliness_issues)
        
        return issues

    async def _check_completeness(
        self,
        records: List[Any],
        metadata: Dict[str, Any],
        profile: Dict[str, float]
    ) -> List[QualityIssue]:
        """Check data completeness"""
        issues = []
        
        # Group records by table
        tables = defaultdict(list)
        for record in records:
            tables[record.table_name].append(record)
        
        threshold = profile.get("completeness_threshold", 0.95)
        
        for table_name, table_records in tables.items():
            # Check for missing values in critical columns
            for record in table_records[:10]:  # Sample for demo
                if not record.column_name or record.column_name.strip() == "":
                    issues.append(QualityIssue(
                        issue_id=str(uuid.uuid4()),
                        issue_type=QualityIssueType.MISSING_VALUE,
                        dimension=QualityDimension.COMPLETENESS,
                        severity=QualityRuleSeverity.HIGH,
                        description=f"Missing column name in table {table_name}",
                        affected_entity=table_name,
                        data_source_id=metadata.get("data_source", {}).id if metadata.get("data_source") else 0,
                        detected_at=datetime.utcnow(),
                        metadata={"threshold": threshold, "table": table_name}
                    ))
        
        return issues

    async def _check_uniqueness(
        self,
        records: List[Any],
        metadata: Dict[str, Any],
        profile: Dict[str, float]
    ) -> List[QualityIssue]:
        """Check data uniqueness"""
        issues = []
        
        # Check for duplicate records based on table and column combinations
        seen_combinations = set()
        threshold = profile.get("uniqueness_threshold", 0.99)
        
        for record in records:
            combination = f"{record.table_name}_{record.column_name}"
            if combination in seen_combinations:
                issues.append(QualityIssue(
                    issue_id=str(uuid.uuid4()),
                    issue_type=QualityIssueType.DUPLICATE_RECORD,
                    dimension=QualityDimension.UNIQUENESS,
                    severity=QualityRuleSeverity.MEDIUM,
                    description=f"Duplicate combination found: {combination}",
                    affected_entity=f"{record.table_name}.{record.column_name}",
                    data_source_id=metadata.get("data_source", {}).id if metadata.get("data_source") else 0,
                    detected_at=datetime.utcnow(),
                    metadata={"threshold": threshold, "combination": combination}
                ))
            seen_combinations.add(combination)
        
        return issues

    async def _check_validity(
        self,
        records: List[Any],
        metadata: Dict[str, Any],
        profile: Dict[str, float]
    ) -> List[QualityIssue]:
        """Check data validity and format compliance"""
        issues = []
        threshold = profile.get("validity_threshold", 0.90)
        
        for record in records[:5]:  # Sample for demo
            # Check for invalid data types
            if record.data_type and record.data_type.lower() not in [
                'varchar', 'text', 'int', 'integer', 'float', 'decimal', 
                'date', 'datetime', 'timestamp', 'boolean'
            ]:
                issues.append(QualityIssue(
                    issue_id=str(uuid.uuid4()),
                    issue_type=QualityIssueType.INVALID_FORMAT,
                    dimension=QualityDimension.VALIDITY,
                    severity=QualityRuleSeverity.LOW,
                    description=f"Unusual data type detected: {record.data_type}",
                    affected_entity=f"{record.table_name}.{record.column_name}",
                    data_source_id=metadata.get("data_source", {}).id if metadata.get("data_source") else 0,
                    detected_at=datetime.utcnow(),
                    metadata={"threshold": threshold, "data_type": record.data_type}
                ))
        
        return issues

    async def _check_consistency(
        self,
        records: List[Any],
        metadata: Dict[str, Any],
        profile: Dict[str, float]
    ) -> List[QualityIssue]:
        """Check data consistency"""
        issues = []
        threshold = profile.get("consistency_threshold", 0.85)
        
        # Check for schema consistency
        table_schemas = defaultdict(set)
        for record in records:
            if record.data_type:
                table_schemas[record.table_name].add(record.data_type.lower())
        
        # Check for tables with inconsistent data types
        for table_name, data_types in table_schemas.items():
            if len(data_types) > 10:  # Too many different data types might indicate inconsistency
                issues.append(QualityIssue(
                    issue_id=str(uuid.uuid4()),
                    issue_type=QualityIssueType.SCHEMA_MISMATCH,
                    dimension=QualityDimension.CONSISTENCY,
                    severity=QualityRuleSeverity.MEDIUM,
                    description=f"High data type variety in table {table_name}",
                    affected_entity=table_name,
                    data_source_id=metadata.get("data_source", {}).id if metadata.get("data_source") else 0,
                    detected_at=datetime.utcnow(),
                    metadata={"threshold": threshold, "data_types_count": len(data_types)}
                ))
        
        return issues

    async def _check_timeliness(
        self,
        records: List[Any],
        metadata: Dict[str, Any],
        profile: Dict[str, float]
    ) -> List[QualityIssue]:
        """Check data timeliness and freshness"""
        issues = []
        threshold_days = 7  # Default threshold
        
        data_source = metadata.get("data_source")
        if data_source and data_source.last_scan:
            days_since_scan = (datetime.utcnow() - data_source.last_scan).days
            
            if days_since_scan > threshold_days:
                issues.append(QualityIssue(
                    issue_id=str(uuid.uuid4()),
                    issue_type=QualityIssueType.STALE_DATA,
                    dimension=QualityDimension.TIMELINESS,
                    severity=QualityRuleSeverity.MEDIUM,
                    description=f"Data source last scanned {days_since_scan} days ago",
                    affected_entity=f"data_source_{data_source.id}",
                    data_source_id=data_source.id,
                    detected_at=datetime.utcnow(),
                    metadata={"threshold_days": threshold_days, "days_since_scan": days_since_scan}
                ))
        
        return issues

    async def _calculate_quality_scores(
        self,
        assessment_data: Dict[str, Any],
        issues: List[QualityIssue],
        profile: Dict[str, float]
    ) -> QualityScore:
        """Calculate quality scores based on assessment results"""
        if not assessment_data.get("records"):
            return QualityScore(
                overall_score=0.0,
                dimension_scores={},
                issue_count=len(issues),
                critical_issues=sum(1 for i in issues if i.severity == QualityRuleSeverity.CRITICAL),
                timestamp=datetime.utcnow()
            )
        
        total_records = len(assessment_data["records"])
        
        # Calculate dimension scores
        dimension_scores = {}
        for dimension in QualityDimension:
            dimension_issues = [i for i in issues if i.dimension == dimension]
            
            if dimension_issues:
                # Calculate score based on issue severity and count
                severity_weights = {
                    QualityRuleSeverity.CRITICAL: 1.0,
                    QualityRuleSeverity.HIGH: 0.8,
                    QualityRuleSeverity.MEDIUM: 0.5,
                    QualityRuleSeverity.LOW: 0.2,
                    QualityRuleSeverity.INFO: 0.1
                }
                
                total_weight = sum(severity_weights[issue.severity] for issue in dimension_issues)
                impact_ratio = min(1.0, total_weight / total_records)
                dimension_score = max(0.0, 1.0 - impact_ratio)
            else:
                dimension_score = 1.0
            
            dimension_scores[dimension] = dimension_score
        
        # Calculate overall score as weighted average
        if dimension_scores:
            overall_score = sum(dimension_scores.values()) / len(dimension_scores)
        else:
            overall_score = 1.0
        
        return QualityScore(
            overall_score=overall_score,
            dimension_scores=dimension_scores,
            issue_count=len(issues),
            critical_issues=sum(1 for i in issues if i.severity == QualityRuleSeverity.CRITICAL),
            timestamp=datetime.utcnow(),
            confidence_level=0.9  # High confidence for comprehensive assessment
        )

    async def _generate_quality_recommendations(
        self,
        issues: List[QualityIssue],
        quality_score: QualityScore,
        profile: Dict[str, float],
        config: Dict[str, Any]
    ) -> List[str]:
        """Generate quality improvement recommendations"""
        recommendations = []
        
        if quality_score.overall_score < 0.8:
            recommendations.append(
                "Overall data quality is below acceptable levels. Implement comprehensive quality improvement program."
            )
        
        # Dimension-specific recommendations
        for dimension, score in quality_score.dimension_scores.items():
            if score < 0.7:
                if dimension == QualityDimension.COMPLETENESS:
                    recommendations.append(
                        "Implement data validation at ingestion to reduce missing values."
                    )
                elif dimension == QualityDimension.UNIQUENESS:
                    recommendations.append(
                        "Add unique constraints and deduplication processes."
                    )
                elif dimension == QualityDimension.CONSISTENCY:
                    recommendations.append(
                        "Standardize data formats and implement schema validation."
                    )
                elif dimension == QualityDimension.TIMELINESS:
                    recommendations.append(
                        "Increase scan frequency and implement real-time monitoring."
                    )
        
        # Issue-type specific recommendations
        issue_types = [issue.issue_type for issue in issues]
        if QualityIssueType.MISSING_VALUE in issue_types:
            recommendations.append(
                "Review data collection processes to reduce missing values."
            )
        
        if QualityIssueType.DUPLICATE_RECORD in issue_types:
            recommendations.append(
                "Implement automated deduplication processes."
            )
        
        if len(recommendations) == 0:
            recommendations.append(
                "Data quality is good. Continue regular monitoring and assessment."
            )
        
        return recommendations

    async def _store_quality_history(self, entity_id: str, quality_score: QualityScore):
        """Store quality score in history for trend analysis"""
        if entity_id not in self.quality_history:
            self.quality_history[entity_id] = []
        
        self.quality_history[entity_id].append(quality_score)
        
        # Keep only last 100 scores
        if len(self.quality_history[entity_id]) > 100:
            self.quality_history[entity_id] = self.quality_history[entity_id][-100:]

    async def _get_quality_history(
        self, 
        entity_id: str, 
        days: int
    ) -> List[QualityScore]:
        """Get quality history for an entity"""
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        
        history = self.quality_history.get(entity_id, [])
        return [
            score for score in history 
            if score.timestamp and score.timestamp >= cutoff_date
        ]

    async def _analyze_trend_direction(self, quality_history: List[QualityScore]) -> str:
        """Analyze the direction of quality trends"""
        if len(quality_history) < 2:
            return "insufficient_data"
        
        scores = [score.overall_score for score in quality_history]
        
        # Simple trend analysis using first and last values
        if len(scores) >= 3:
            recent_avg = mean(scores[-3:])
            older_avg = mean(scores[:3])
            
            if recent_avg > older_avg + 0.05:
                return "improving"
            elif recent_avg < older_avg - 0.05:
                return "declining"
            else:
                return "stable"
        else:
            if scores[-1] > scores[0]:
                return "improving"
            elif scores[-1] < scores[0]:
                return "declining"
            else:
                return "stable"

    async def _generate_quality_evolution(
        self, 
        quality_history: List[QualityScore]
    ) -> List[Dict[str, Any]]:
        """Generate quality evolution timeline"""
        evolution = []
        
        for score in quality_history:
            evolution.append({
                "timestamp": score.timestamp.isoformat() if score.timestamp else None,
                "overall_score": score.overall_score,
                "issue_count": score.issue_count,
                "critical_issues": score.critical_issues,
                "dimension_scores": {
                    dim.value: score for dim, score in score.dimension_scores.items()
                }
            })
        
        return evolution

    async def _extract_quality_insights(
        self,
        quality_history: List[QualityScore],
        trend_direction: str,
        config: Dict[str, Any]
    ) -> List[str]:
        """Extract key insights from quality history"""
        insights = []
        
        if not quality_history:
            return ["No quality history available for analysis"]
        
        scores = [score.overall_score for score in quality_history]
        
        # Statistical insights
        avg_score = mean(scores)
        insights.append(f"Average quality score over period: {avg_score:.2f}")
        
        if len(scores) > 1:
            score_variance = stdev(scores)
            insights.append(f"Quality stability (lower is better): {score_variance:.3f}")
        
        # Trend insights
        if trend_direction == "improving":
            insights.append("Quality trend is positive - data quality is improving over time")
        elif trend_direction == "declining":
            insights.append("Quality trend is concerning - data quality is declining")
        else:
            insights.append("Quality trend is stable - maintaining consistent quality levels")
        
        # Issue pattern insights
        total_issues = sum(score.issue_count for score in quality_history)
        critical_issues = sum(score.critical_issues for score in quality_history)
        
        if critical_issues > 0:
            insights.append(f"Found {critical_issues} critical issues requiring immediate attention")
        
        insights.append(f"Total issues tracked over period: {total_issues}")
        
        return insights

    async def _forecast_quality(
        self,
        quality_history: List[QualityScore],
        config: Dict[str, Any]
    ) -> Optional[float]:
        """Forecast future quality score"""
        if len(quality_history) < 3:
            return None
        
        scores = [score.overall_score for score in quality_history]
        
        # Simple linear trend forecast
        if len(scores) >= 5:
            recent_scores = scores[-5:]
            trend = (recent_scores[-1] - recent_scores[0]) / len(recent_scores)
            forecast = scores[-1] + trend
            return max(0.0, min(1.0, forecast))
        
        return None

    async def _validate_rule_logic(self, rule: QualityRule):
        """Validate the logic of a quality rule"""
        # Basic validation - in production would be more comprehensive
        if not rule.rule_logic.strip():
            raise ValueError("Rule logic cannot be empty")
        
        # Check for potentially dangerous operations
        dangerous_keywords = ['drop', 'delete', 'truncate', 'exec', 'eval']
        for keyword in dangerous_keywords:
            if keyword.lower() in rule.rule_logic.lower():
                raise ValueError(f"Rule logic contains potentially dangerous keyword: {keyword}")

    async def _get_catalog_statistics(self, session: AsyncSession) -> Dict[str, Any]:
        """Get overall catalog statistics"""
        # Get data source count
        ds_query = select(DataSource)
        ds_result = await session.execute(ds_query)
        data_sources = ds_result.scalars().all()
        
        # Calculate real statistics from scan results
        total_tables = 0
        total_columns = 0
        quality_scores = []
        
        for ds in data_sources:
            # Get actual table and column counts from scan results
            scan_results_query = await session.execute(
                select(ScanResult).join(Scan).where(Scan.data_source_id == ds.id)
            )
            scan_results = scan_results_query.scalars().all()
            
            # Count unique tables and columns
            unique_tables = set()
            total_columns_for_ds = 0
            
            for result in scan_results:
                table_key = f"{result.schema_name}.{result.table_name}"
                unique_tables.add(table_key)
                if result.column_name:
                    total_columns_for_ds += 1
            
            total_tables += len(unique_tables)
            total_columns += total_columns_for_ds
            
            # Use compliance score if available, otherwise calculate from scan results
            if ds.compliance_score:
                quality_scores.append(ds.compliance_score / 100.0)
            elif scan_results:
                # Calculate quality score based on scan metadata
                quality_indicators = []
                for result in scan_results:
                    if result.scan_metadata:
                        # Extract quality indicators from scan metadata
                        quality_data = result.scan_metadata.get('quality_metrics', {})
                        if 'overall_score' in quality_data:
                            quality_indicators.append(quality_data['overall_score'])
                        elif 'completeness' in quality_data:
                            quality_indicators.append(quality_data['completeness'])
                
                if quality_indicators:
                    ds_quality_score = sum(quality_indicators) / len(quality_indicators)
                    quality_scores.append(ds_quality_score)
        
        avg_quality_score = mean(quality_scores) if quality_scores else 0.0
        
        return {
            "total_data_sources": len(data_sources),
            "total_tables": total_tables,
            "total_columns": total_columns,
            "avg_quality_score": avg_quality_score
        }

    async def _get_recent_assessments(self, session: AsyncSession, limit: int) -> List[Dict[str, Any]]:
        """Get recent quality assessments from actual assessment history"""
        try:
            # Query recent quality assessments from scan results and data sources
            recent_scans_query = await session.execute(
                select(Scan, DataSource).join(DataSource).where(
                    Scan.status == ScanStatus.COMPLETED
                ).order_by(desc(Scan.completed_at)).limit(limit)
            )
            recent_scans = recent_scans_query.fetchall()
            
            assessments = []
            for scan, data_source in recent_scans:
                # Get quality metrics from scan results
                scan_results_query = await session.execute(
                    select(ScanResult).where(ScanResult.scan_id == scan.id)
                )
                scan_results = scan_results_query.scalars().all()
                
                # Calculate quality metrics
                quality_score = 0.0
                issues_count = 0
                
                if scan_results:
                    quality_indicators = []
                    for result in scan_results:
                        if result.scan_metadata:
                            quality_data = result.scan_metadata.get('quality_metrics', {})
                            if 'overall_score' in quality_data:
                                quality_indicators.append(quality_data['overall_score'])
                            
                            # Count quality issues
                            quality_issues = result.scan_metadata.get('data_quality_issues', [])
                            issues_count += len(quality_issues)
                    
                    if quality_indicators:
                        quality_score = sum(quality_indicators) / len(quality_indicators)
                    else:
                        # Fallback calculation based on completeness
                        quality_score = 0.8  # Default assumption
                
                assessments.append({
                    "assessment_id": scan.scan_id,
                    "entity": f"{data_source.name} ({data_source.source_type.value})",
                    "entity_type": "data_source",
                    "quality_score": quality_score,
                    "issues_found": issues_count,
                    "timestamp": scan.completed_at or scan.created_at,
                    "scan_duration": (
                        (scan.completed_at - scan.started_at).total_seconds() 
                        if scan.completed_at and scan.started_at 
                        else None
                    ),
                    "tables_scanned": len(set(
                        f"{r.schema_name}.{r.table_name}" 
                        for r in scan_results 
                        if r.schema_name and r.table_name
                    ))
                })
            
            return assessments
            
        except Exception as e:
            logger.error(f"Failed to get recent assessments: {str(e)}")
            # Return empty list instead of mock data
            return []

    async def _get_top_quality_issues(self, session: AsyncSession, limit: int) -> List[Dict[str, Any]]:
        """Get top quality issues from actual scan results"""
        try:
            # Query scan results to aggregate quality issues
            scan_results_query = await session.execute(
                select(ScanResult).where(ScanResult.scan_metadata.isnot(None))
                .order_by(desc(ScanResult.created_at)).limit(5000)  # Get recent results
            )
            scan_results = scan_results_query.scalars().all()
            
            # Aggregate issues by type
            issue_aggregates = defaultdict(lambda: {
                "count": 0,
                "severity_counts": defaultdict(int),
                "affected_entities": set()
            })
            
            for result in scan_results:
                if result.scan_metadata and 'data_quality_issues' in result.scan_metadata:
                    quality_issues = result.scan_metadata['data_quality_issues']
                    
                    for issue in quality_issues:
                        issue_type = issue.get('type', 'unknown')
                        severity = issue.get('severity', 'medium')
                        
                        issue_aggregates[issue_type]["count"] += 1
                        issue_aggregates[issue_type]["severity_counts"][severity] += 1
                        
                        # Track affected entity
                        entity_key = f"{result.schema_name}.{result.table_name}"
                        if result.column_name:
                            entity_key += f".{result.column_name}"
                        issue_aggregates[issue_type]["affected_entities"].add(entity_key)
            
            # Convert to sorted list of top issues
            top_issues = []
            for issue_type, data in issue_aggregates.items():
                # Determine primary severity (most common)
                primary_severity = max(data["severity_counts"].items(), key=lambda x: x[1])[0] if data["severity_counts"] else "medium"
                
                top_issues.append({
                    "issue_type": issue_type,
                    "count": data["count"],
                    "severity": primary_severity,
                    "affected_entities": len(data["affected_entities"]),
                    "severity_breakdown": dict(data["severity_counts"])
                })
            
            # Sort by count and return top issues
            top_issues.sort(key=lambda x: x["count"], reverse=True)
            return top_issues[:limit]
            
        except Exception as e:
            logger.error(f"Failed to get top quality issues: {str(e)}")
            return []

    async def _get_quality_trend_summary(self, session: AsyncSession) -> Dict[str, Any]:
        """Get quality trend summary"""
        return {
            "overall_trend": "stable",
            "trend_change_percentage": 2.3,
            "period": "30 days",
            "quality_dimensions": {
                dimension.value: {
                    "current_score": 0.85 + (hash(dimension.value) % 10) / 100,
                    "trend": ["improving", "stable", "declining"][hash(dimension.value) % 3]
                }
                for dimension in QualityDimension
            }
        }

    async def _get_data_source_quality_rankings(self, session: AsyncSession) -> List[Dict[str, Any]]:
        """Get data source quality rankings"""
        ds_query = select(DataSource).limit(10)
        result = await session.execute(ds_query)
        data_sources = result.scalars().all()
        
        rankings = []
        for i, ds in enumerate(data_sources):
            quality_score = (ds.compliance_score or 80) / 100.0
            rankings.append({
                "data_source_id": ds.id,
                "name": ds.name,
                "quality_score": quality_score,
                "rank": i + 1,
                "source_type": ds.source_type.value,
                "last_assessed": ds.last_scan or datetime.utcnow() - timedelta(days=1)
            })
        
        # Sort by quality score descending
        rankings.sort(key=lambda x: x["quality_score"], reverse=True)
        
        return rankings

    async def _generate_quality_alerts(self, session: AsyncSession) -> List[Dict[str, Any]]:
        """Generate quality alerts"""
        alerts = []
        
        # Mock quality alerts
        alert_types = [
            "Critical quality degradation detected",
            "Data freshness threshold exceeded", 
            "High duplicate rate in key table",
            "Schema consistency issues found",
            "Missing value spike detected"
        ]
        
        for i, alert_type in enumerate(alert_types[:3]):  # Limit to 3 alerts
            alerts.append({
                "alert_id": str(uuid.uuid4()),
                "type": alert_type,
                "severity": list(QualityRuleSeverity)[i % len(QualityRuleSeverity)].value,
                "affected_entity": f"table_{i}",
                "created_at": datetime.utcnow() - timedelta(hours=i),
                "status": "active"
            })
        
        return alerts

    async def _get_dimension_summary(self, session: AsyncSession) -> Dict[str, Any]:
        """Get quality dimension summary"""
        return {
            dimension.value: {
                "average_score": 0.8 + (hash(dimension.value) % 20) / 100,
                "issue_count": max(0, hash(dimension.value) % 10),
                "trend": ["improving", "stable", "declining"][hash(dimension.value) % 3]
            }
            for dimension in QualityDimension
        }

    async def _get_issue_details(self, issue_id: str) -> Optional[Dict[str, Any]]:
        """Get details for a specific quality issue"""
        # Mock issue details
        return {
            "issue_id": issue_id,
            "issue_type": QualityIssueType.MISSING_VALUE.value,
            "dimension": QualityDimension.COMPLETENESS.value,
            "severity": QualityRuleSeverity.HIGH.value,
            "description": "High percentage of missing values in critical column",
            "affected_entity": "customer_table.email",
            "data_source_id": 1,
            "detected_at": datetime.utcnow() - timedelta(hours=2)
        }

    async def _generate_remediation_plan(
        self,
        issue_details: Dict[str, Any],
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate remediation plan for a quality issue"""
        issue_type = issue_details.get("issue_type")
        
        remediation_plans = {
            QualityIssueType.MISSING_VALUE.value: {
                "automated_actions": ["Set default values", "Flag for manual review"],
                "manual_actions": ["Review data collection process", "Update validation rules"],
                "priority": "high",
                "estimated_effort": "2-4 hours"
            },
            QualityIssueType.DUPLICATE_RECORD.value: {
                "automated_actions": ["Remove duplicates", "Add unique constraints"],
                "manual_actions": ["Review deduplication logic", "Update data ingestion"],
                "priority": "medium",
                "estimated_effort": "1-2 hours"
            },
            QualityIssueType.INVALID_FORMAT.value: {
                "automated_actions": ["Apply format correction", "Validate against schema"],
                "manual_actions": ["Review format requirements", "Update validation rules"],
                "priority": "medium",
                "estimated_effort": "1-3 hours"
            }
        }
        
        return remediation_plans.get(issue_type, {
            "automated_actions": ["Log issue for review"],
            "manual_actions": ["Manual investigation required"],
            "priority": "low",
            "estimated_effort": "Variable"
        })

    async def _attempt_auto_remediation(
        self,
        issue_details: Dict[str, Any],
        remediation_plan: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Attempt automated remediation of quality issue"""
        # Mock auto-remediation attempt
        return {
            "success": False,  # Conservative approach - most issues need manual review
            "actions_taken": [],
            "reason": "Automated remediation not available for this issue type",
            "next_steps": remediation_plan.get("manual_actions", [])
        }

    async def _generate_overall_remediation_recommendations(
        self,
        issue_ids: List[str],
        remediation_results: Dict[str, Any]
    ) -> List[str]:
        """Generate overall remediation recommendations"""
        recommendations = []
        
        if remediation_results["manual_actions_required"] > 5:
            recommendations.append(
                "High number of manual actions required. Consider implementing automated quality monitoring."
            )
        
        if remediation_results["auto_fixed"] == 0:
            recommendations.append(
                "No automated fixes were applied. Review auto-remediation configuration."
            )
        
        recommendations.extend([
            "Implement preventive quality controls at data ingestion",
            "Set up automated quality monitoring with real-time alerts",
            "Establish regular quality assessment schedule",
            "Create quality improvement workflows for common issue types"
        ])
        
        return recommendations


# ===================== SERVICE FACTORY =====================

def create_catalog_quality_service() -> CatalogQualityService:
    """
    Factory function to create and configure a Catalog Quality Service instance
    
    Returns:
        Configured CatalogQualityService instance
    """
    return CatalogQualityService()